trtexec --onnx=/workspace/model/gpu/gpu-int4-rtn-block-32/model.onnx \
        --saveEngine=/workspace/model/model.plan \
        --fp16 \
        --verbose \
        --dumpProfile \
        --best \
        --timingCacheFile=/workspace/model/timing_cache.cache

docker network create triton-net




cd /home/accenture/git/triton-fastapi/inference_server/models/phi4_reasoning && \
      sudo docker run --gpus all -it --rm --network triton-net \
      -v $(pwd):/workspace/model \
      nvcr.io/nvidia/tritonserver:25.06-py3 \
      /bin/bash -c "pip install tritonclient[http] && cd /workspace/model && /bin/bash"