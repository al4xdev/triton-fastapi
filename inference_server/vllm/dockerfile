FROM nvidia/cuda:12.6.1-cudnn-devel-ubuntu24.04

RUN apt update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        python3 \
        python3-pip \
        python3-dev \
        python3-venv \
        git \
        curl \
        ca-certificates \
        locales \
        libgl1 \
        libgl-dev \
        ffmpeg \
        libsm6 \
        libxext6 \
        build-essential &&\
    rm -rf /var/lib/apt/lists/* && \
    locale-gen en_US.UTF-8 && \
    update-alternatives --install /usr/bin/python python /usr/bin/python3 1



ENV LANG=en_US.UTF-8
ENV LANGUAGE=en_US:en
ENV LC_ALL=en_US.UTF-8


COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv
ENV PATH="/usr/local/bin:${PATH}"


WORKDIR /app

RUN uv venv .venv && \
    PATH="/app/.venv/bin:${PATH}" uv pip install --no-cache-dir \
        torch==2.6.0+cu126 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126 && \
    uv pip install flashinfer-python -i https://flashinfer.ai/whl/cu126/torch2.6/ && \
    uv pip install --upgrade pip setuptools setuptools-scm huggingface-hub numpy && \
    rm -rf /root/.cache/uv/*

# RUN uv venv .venv
# ENV PATH="/app/.venv/bin:${PATH}"
# ENV UV_LINK_MODE=copy


# RUN uv pip install --no-cache-dir \
#     torch==2.6.0+cu126 \
#     torchvision \
#     torchaudio \
#     --index-url https://download.pytorch.org/whl/cu126 && \
#     uv pip install flashinfer-python -i https://flashinfer.ai/whl/cu126/torch2.6/ && \
#     rm -rf /root/.cache/uv/*

    
COPY . /app/vllm
WORKDIR /app/vllm


ENV SETUPTOOLS_SCM_PRETEND_VERSION_FOR_VLLM=0.1.0
ENV SETUPTOOLS_SCM_PRETEND_VERSION=0.1.0
ENV VLLM_USE_PRECOMPILED=1

RUN uv pip install --upgrade pip setuptools setuptools-scm huggingface-hub numpy && \
    uv pip install --no-cache-dir --editable . && \
    if [ ! -f /app/models/GLM-4.1V-9B-Thinking-AWQ/config.json ]; then \
        hf download QuantTrio/GLM-4.1V-9B-Thinking-AWQ --local-dir /app/models/GLM-4.1V-9B-Thinking-AWQ/ ; \
    fi

WORKDIR /app

EXPOSE 8000

CMD ["python3", "-m", "vllm.entrypoints.openai.api_server", \
    "--model", "/app/models/GLM-4.1V-9B-Thinking-AWQ", \
    "--port", "8000", \
    "--enable-prefix-caching", \
    "--swap-space", "7", \
    "--max-num-seqs", "512", \
    "--max-model-len", "65536", \
    "--max-seq-len-to-capture", "65536", \
    "--gpu-memory-utilization", "0.9", \
    "--tensor-parallel-size", "1", \
    "--reasoning-parser", "glm4_moe"]