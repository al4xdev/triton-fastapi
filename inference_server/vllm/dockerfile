FROM nvidia/cuda:12.6.1-cudnn-runtime-ubuntu22.04

RUN apt update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        python3 \
        python3-pip \
        python3-dev \
        python3-venv \
        git \
        curl \
        ca-certificates \
        locales \
        libgl1 \
        libgl-dev \
        ffmpeg \
        libsm6 \
        libxext6 \
        build-essential &&\
    rm -rf /var/lib/apt/lists/*

RUN locale-gen en_US.UTF-8

ENV LANG=en_US.UTF-8
ENV LANGUAGE=en_US:en
ENV LC_ALL=en_US.UTF-8

RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1

COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

ENV PATH="/usr/local/bin:${PATH}"

WORKDIR /app

RUN uv venv .venv
ENV PATH="/app/.venv/bin:${PATH}"
ENV UV_LINK_MODE=copy

RUN uv pip install --no-cache-dir \
    torch==2.6.0+cu126 \
    torchvision \
    torchaudio \
    --index-url https://download.pytorch.org/whl/cu126/ && \
    uv pip install flashinfer-python -i https://flashinfer.ai/whl/cu126/torch2.6/

COPY . /app/vllm
WORKDIR /app/vllm
ENV SETUPTOOLS_SCM_PRETEND_VERSION_FOR_VLLM=0.1.0

ENV VLLM_USE_PRECOMPILED=1
RUN pip install --upgrade pip setuptools setuptools-scm
RUN pip install --no-cache-dir --editable .

RUN pip install huggingface-cli

RUN hf QuantTrio/GLM-4.1V-9B-Thinking-AWQ --local-dir /app/models/GLM-4.1V-9B-Thinking-AWQ/

RUN hf cache delete

WORKDIR /app

EXPOSE 8000

ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]

CMD ["--host", "0.0.0.0", \
     "--port", "8000", \
     "--enable-prefix-caching", \
     "--swap-space", "7", \
     "--max-num-seqs", "512", \
     "--max-model-len", "65536", \
     "--max-seq-len-to-capture", "65536", \
     "--gpu-memory-utilization", "0.9", \
     "--tensor-parallel-size", "1", \
     "--limit-mm-per-prompt", "{\"image\":32}", \
     "--reasoning-parser glm4_moe"]
